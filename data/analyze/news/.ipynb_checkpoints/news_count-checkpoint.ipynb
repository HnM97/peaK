{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a19d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 232 ms, sys: 293 ms, total: 526 ms\n",
      "Wall time: 27.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "import pyspark\n",
    "import string\n",
    "from pecab import PeCab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import pandas_udf, monotonically_increasing_id, regexp_replace, sum, asc, \\\n",
    "                                   arrays_zip, col, explode, sort_array, struct, lit, PandasUDFType, coalesce, \\\n",
    "                                    concat\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType, ArrayType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# local[*] : 모든 코어를 사용하겠다, local[4] : 4개의 코어를 사용하겠다.\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"WordCount\")\\\n",
    "        .config(\"spark.driver.memory\", \"6g\")\\\n",
    "        .getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "\n",
    "# date = sys.argv[1]\n",
    "# hour = sys.argv[2]\n",
    "\n",
    "# data_name= f\"{date}_{hour}_NC.txt\"\n",
    "# origin_df = spark.read.options(header='True', inferSchema='True', delimiter='|',quote='\"', multiLine=\"true\") \\\n",
    "#                         .option(\"encoding\", \"UTF-8\").csv(f\"file:///home/j8a507/watcher/crawled/news/{data_name}\")\n",
    "\n",
    "date = \"230405\"\n",
    "hour = \"14\"\n",
    "data_name= \"230405_14_NC.txt\"\n",
    "# origin_df = spark.read.options(header='True', inferSchema='True', delimiter='|',quote='\"', multiLine=\"true\") \\\n",
    "#                         .option(\"encoding\", \"UTF-8\").txt(f\"file:///home/j8a507/cluster/news/{data_name}\")\n",
    "origin_df = spark.read.json(f\"file:///home/j8a507/cluster/news/{data_name}\")\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def rm_space_in_quote_udf(x: pd.Series) -> pd.Series:\n",
    "    def rm_space_in_quote(string):\n",
    "        encoded_string = string.encode('utf-8')\n",
    "        decoded_string = encoded_string.decode('utf-8')\n",
    "        no_space_string = re.sub(r\"'[^']*'\", lambda m: m.group().replace(\" \", \"\"), decoded_string)\n",
    "        return no_space_string\n",
    "\n",
    "    return x.apply(rm_space_in_quote)\n",
    "\n",
    "df =  origin_df.withColumn(\"total_content\", concat(col(\"content\"), col(\"summary\"), col(\"title\")))\\\n",
    "    .select(col(\"index\"),col(\"idol\"),col(\"total_content\"))\n",
    "\n",
    "df = df.withColumn(\"total_content\", rm_space_in_quote_udf(df[\"total_content\"]))\n",
    "\n",
    "user_dict= {'(여자)아이들', '(G)I-DLE', 'AB6IX', '에이비식스', 'ASTRO', '아스트로', 'ATBO', '에이티비오', 'ATEEZ', '에이티즈', 'Apink', '에이핑크', 'BAE173', '비에이이일칠삼/173', 'BDC', '비디씨', 'BLACKPINK', '블랙핑크', 'CIX', '씨아이엑스', 'CLASS:y', '클라씨', 'CNBLUE',\\\n",
    "            'CRAVITY', '크래비티', 'DAY6', '데이식스', 'DRIPPIN', '드리핀', 'ENHYPEN', '엔하이픈', 'EPEX', '이펙스', 'EVERGLOW', '에버글로우', 'EXID', '이엑스아이디', 'EXO', '엑소', 'GOT7', '갓세븐', 'ITZY', '있지', 'IVE', '아이브', 'KARD', '카드', 'LESSERAFIM', '르세라핌', 'LIGHTSUM', '라잇썸', 'MCND', '엠씨엔디', 'NCT', '엔시티', \\\n",
    "            'NCT127', '엔시티127', 'NCTDREAM', '엔시티드림', 'NMIXX', '엔믹스', 'NewJeans', '뉴진스', 'P1Harmony', '피원하모니', 'RedVelvet', '레드벨벳', 'SF9', '에스에프나인', 'SHINee', '샤이니', 'StrayKids', '스트레이키즈', 'TEMPEST', '템페스트', 'TREASURE', '트레저', 'TWICE', '트와이스', 'VERIVERY', '베리베리', 'VICTON', '빅톤', 'WINNER',\\\n",
    "            '위너', 'Weeekly', '위클리', 'XdinaryHeroes', '엑스디너리히어로즈', 'YOUNITE', '유나이트', 'aespa', '에스파', 'iKON', '아이콘', '골든차일드', ' GNCD', '공원소녀', 'GirlsInThePark', 'GWSN', '다크비', 'DKB', '더보이즈', 'THEBOYZ', 'LABOUM', '라붐', '레이디스코드', \"LADIES'CODE\", '로켓펀치', ' RCPC', '마마무', 'MAMAMOO', '모모랜드',\\\n",
    "            'MOMOLAND', '몬스타엑스', 'MONSTAX', '미래소년', 'MIRAE', '박지훈', 'PARKJIHOON', '방탄소년단', 'BTS', '브레이브걸스', 'BraveGirls', '블락비', 'BlockB', 'BLACKSWAN', '블랙스완', 'BTOB', '비투비', 'VIXX', '빅스', '세븐틴', 'SEVENTEEN', '소녀시대', \"Girls'Generation\", '아이유', ' IU ', '앤씨아', 'NC.A', ' 업텐션', ' UP10TION', '엔플라잉',\\\n",
    "            'N.Flying', '오마이걸', 'OHMYGIRL', '온앤오프', 'ONF', '우주소녀', 'WJSN', '원어스', 'ONEUS', 'ONEWE', '원위', '위키미키', 'WekiMeki', '유키스', 'U-KISS', '이달의소녀', 'LOOΠΔ', '인피니트', 'INFINITE', '주니엘', 'JUNIEL', '청하', 'CHUNGHA', '체리블렛', 'CherryBullet', '퀸즈아이', 'QueenzEye', '투모로우바이투게더', 'TXT', 'TOMORROWXTOMORROW'\\\n",
    "            '틴탑', 'TEENTOP', '퍼플키스', 'PURPLEKISS', '펜타곤', 'PENTAGON', '프로미스나인', 'fromis_9', '걸그룹', '소속사','엔터테인먼트'}\n",
    "\n",
    "\n",
    "# 사용자 정의사전 만들기\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def make_user_dict_udf(x: pd.Series) -> pd.Series:\n",
    "    def make_user_dict(string):\n",
    "        encoded_string = string.encode('utf-8')\n",
    "        decoded_string = encoded_string.decode('utf-8')\n",
    "        substrings = re.findall(r\"'(.*?)'\", decoded_string)\n",
    "        return substrings\n",
    "    return x.apply(make_user_dict)\n",
    "\n",
    "df_dict = df.select(\"total_content\")\n",
    "df_dict= df_dict.withColumn(\"total_content\", make_user_dict_udf(df[\"total_content\"]))\n",
    "# df_dict.collect()\n",
    "for w in df_dict.toLocalIterator():\n",
    "    user_dict.update(w[\"total_content\"])\n",
    "\n",
    "pecab = PeCab(user_dict=user_dict)\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def clean_str_udf(x: pd.Series) -> pd.Series:\n",
    "    def clean_str(x):\n",
    "        punc = string.punctuation\n",
    "        for ch in punc:\n",
    "            x = x.replace(ch, '')\n",
    "        return x\n",
    "    return x.apply(clean_str)\n",
    "\n",
    "df = df.withColumn(\"total_content\", clean_str_udf(df[\"total_content\"]))\n",
    "\n",
    "def set_words(words):\n",
    "    counter = Counter(words)\n",
    "    return counter.most_common(10)\n",
    "\n",
    "def nouns_row(x):\n",
    "    content = x[1]\n",
    "    nouns = pecab.nouns(content)\n",
    "    return (x[0], nouns)\n",
    "\n",
    "def set_words_row(x):\n",
    "    keywords = x[1]\n",
    "    counter = Counter(keywords)\n",
    "    return (x[0], counter.most_common(10))\n",
    "\n",
    "words_rdd = df.rdd\n",
    "words_rdd = words_rdd.map(lambda x : nouns_row(x))\n",
    "words_rdd = words_rdd.map(lambda x: set_words_row(x))\n",
    "\n",
    "total_rdd = words_rdd.flatMap(lambda x: x[1]).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending=False)\n",
    "words_rdd = words_rdd.zipWithIndex()\n",
    "\n",
    "# Replace \"/path/to/partitioned/data\" with the path to your partitioned data in HDFS\n",
    "hdfs_path_keyword = f\"output/NK/{date}_{hour}_NK.txt\"\n",
    "hdfs_path_total = f\"output/TK/{date}_{hour}_TK.txt\"\n",
    "hdfs_path_count = f\"output/IDK/{date}_{hour}_IDK.txt\"\n",
    "\n",
    "# Replace \"/path/to/local/directory\" with the path to your local directory\n",
    "local_path_keyword = f\"file:///home/j8a507/watcher/analyzed/news/NK/{date}_{hour}_NK.txt\"\n",
    "local_path_total = f\"file:///home/j8a507/watcher/analyzed/news/TK/{date}_{hour}_TK.txt\"\n",
    "local_path_count = f\"file:///home/j8a507/watcher/analyzed/news/IDK/{date}_{hour}_IDK.txt\"\n",
    "\n",
    "words_rdd = words_rdd.map(lambda x: {\"index\": x[1], \"idol\": x[0][0], \"keywords\": x[0][1]})\n",
    "words_json = words_rdd.collect()\n",
    "words_json = json.dumps(words_json, ensure_ascii=False)\n",
    "\n",
    "final_words_rdd = sc.parallelize([words_json])\n",
    "final_words_rdd.saveAsTextFile(hdfs_path_keyword)\n",
    "\n",
    "subprocess.check_call([\"hdfs\", \"dfs\", \"-getmerge\", hdfs_path_keyword, local_path_keyword])\n",
    "\n",
    "\n",
    "total_dict = total_rdd.collect()\n",
    "total_json = json.dumps(total_dict, ensure_ascii=False)\n",
    "\n",
    "\n",
    "final_total_rdd = sc.parallelize([total_json])\n",
    "final_total_rdd.saveAsTextFile(hdfs_path_total)\n",
    "\n",
    "cdf = origin_df.groupBy(\"idol\").count()\n",
    "cdf.write.option(\"header\", True).csv(hdfs_path_count)\n",
    "\n",
    "subprocess.check_call([\"hdfs\", \"dfs\", \"-getmerge\", hdfs_path_count, local_path_count])\n",
    "subprocess.check_call([\"hdfs\", \"dfs\", \"-getmerge\", hdfs_path_total, local_path_total])\n",
    "\n",
    "\n",
    "words_df = words_rdd.toDF([\"idol\",\"index\", \"keywords\"])\n",
    "df_exploded = words_df.select(col(\"idol\"), explode(col(\"keywords\")).alias(\"keyword\"))\n",
    "\n",
    "idol_keyword_counts = df_exploded.groupBy(col(\"idol\"), col(\"keyword._1\").alias(\"keyword\")).agg(F.sum(col(\"keyword._2\")).alias(\"count\"))\n",
    "\n",
    "result = (\n",
    "    idol_keyword_counts\n",
    "    .sort(F.desc(\"count\"))\n",
    "    .groupBy(\"idol\")\n",
    "    .agg(F.collect_list(F.struct(col(\"keyword\"), col(\"count\"))).alias(\"keywords\"))\n",
    "    .withColumn(\"keywords\", F.expr(\"slice(keywords, 1, 10)\"))\n",
    ")\n",
    "\n",
    "hdfs_path_in = f\"output/IN/{date}_{hour}_IN.txt\"\n",
    "local_path_in = f\"file:///home/j8a507/watcher/analyzed/news/IN/{date}_{hour}_IN.txt\"\n",
    "\n",
    "\n",
    "result_rdd = result.rdd.map(lambda x: {\"idol\": x[0], \"keywords\": x[1]})\n",
    "result_json = result_rdd.collect()\n",
    "result_json = json.dumps(result_json, ensure_ascii=False)\n",
    "final_result_rdd = sc.parallelize([result_json])\n",
    "final_result_rdd.saveAsTextFile(hdfs_path_in)\n",
    "\n",
    "subprocess.check_call([\"hdfs\", \"dfs\", \"-getmerge\", hdfs_path_in, local_path_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "import pyspark\n",
    "import string\n",
    "from pecab import PeCab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import pandas_udf, monotonically_increasing_id, regexp_replace, sum, asc, \\\n",
    "                                   arrays_zip, col, explode, sort_array, struct, lit, PandasUDFType, coalesce \n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType, ArrayType\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# local[*] : 모든 코어를 사용하겠다, local[4] : 4개의 코어를 사용하겠다.\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[3]\")\\\n",
    "        .appName(\"WordCount\")\\\n",
    "        .config(\"spark.driver.memory\", \"2g\")\\\n",
    "        .getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "\n",
    "data_name= \"test_csv\"\n",
    "origin_df = spark.read.options(header='True', inferSchema='True', delimiter='|',quote='\"', multiLine=\"true\").option(\"encoding\", \"UTF-8\").csv(f\"file:///home/j8a507/cluster/news/{data_name}.csv\")\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def rm_space_in_quote_udf(x: pd.Series) -> pd.Series:\n",
    "    def rm_space_in_quote(string):\n",
    "        encoded_string = string.encode('utf-8')\n",
    "        decoded_string = encoded_string.decode('utf-8')\n",
    "        no_space_string = re.sub(r\"'[^']*'\", lambda m: m.group().replace(\" \", \"\"), decoded_string)\n",
    "        return no_space_string\n",
    "\n",
    "    return x.apply(rm_space_in_quote)\n",
    "\n",
    "df = origin_df.withColumn(\"content\", rm_space_in_quote_udf(origin_df[\"content\"]))\\\n",
    "\n",
    "user_dict= {\"뉴진스\",\"트와이스\"}\n",
    "\n",
    "\n",
    "# 사용자 정의사전 만들기\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def make_user_dict_udf(x: pd.Series) -> pd.Series:\n",
    "    def make_user_dict(string):\n",
    "        encoded_string = string.encode('utf-8')\n",
    "        decoded_string = encoded_string.decode('utf-8')\n",
    "        substrings = re.findall(r\"'(.*?)'\", decoded_string)\n",
    "        return substrings\n",
    "    return x.apply(make_user_dict)\n",
    "\n",
    "df_dict = df.select(\"content\")\n",
    "df_dict= df_dict.withColumn(\"content\", make_user_dict_udf(df[\"content\"]))\n",
    "# df_dict.collect()\n",
    "for w in df_dict.toLocalIterator():\n",
    "    user_dict.update(w[\"content\"])\n",
    "\n",
    "pecab = PeCab(user_dict=user_dict)\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def clean_str_udf(x: pd.Series) -> pd.Series:\n",
    "    def clean_str(x):\n",
    "        punc = string.punctuation\n",
    "        for ch in punc:\n",
    "            x = x.replace(ch, '')\n",
    "        return x\n",
    "    return x.apply(clean_str)\n",
    "\n",
    "df = df.withColumn(\"content\", clean_str_udf(df[\"content\"])) \\\n",
    "        .withColumn(\"title\", clean_str_udf(df[\"title\"])) \\\n",
    "        .withColumn(\"summary\", clean_str_udf(df[\"summary\"])) \n",
    "\n",
    " df.groupBy(\"content\").agg(sum(df.content+df.summary+ df.title)).show()\n",
    "\n",
    "string_per_news = []\n",
    "df_collector = df.toLocalIterator()\n",
    "for row in df_collector:\n",
    "    string_per_news.append(row[\"content\"]+ \" \" + row[\"title\"] + \" \" + row[\"summary\"])\n",
    "\n",
    "def set_words(words):\n",
    "    counter = Counter(words)\n",
    "    return counter.most_common(10)\n",
    "\n",
    "    \n",
    "words_rdd = sc.parallelize(string_per_news)\n",
    "words_rdd = words_rdd.map(lambda x: pecab.nouns(x))\n",
    "\n",
    "words_rdd = words_rdd.map(lambda x: set_words(x))\n",
    "words_rdd.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c23e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(idol='TWICE', total_content='걸그룹 트와이스가 K팝 여가수 최초로 빌보드위민인뮤직에서 수상했습니다 소속사 JYP엔터테인먼트는 걸그룹 트와이스가 현지시간으로 지난 1일 미국 로스앤젤레스 유튜브 시어터에서 열린 빌보드위민인뮤직 시상식에 참석해 브레이크스루아티스트 부문에서 수상했다고 밝혔습니다 브레이크스루아티스트 부문은 음악 시장에서 의미있는 도전을 해내고 두각을 드러낸 가수에게 주는 상입니다빌보드가 주최하는 이 시상식은 한 해 음악 산업에 큰 영향을 끼친 여성 아티스트와 크리에이터 프로듀서 등에 시상합니다신새롬 기자 romiynacokr연합뉴스TV 기사문의 및 제보  카톡라인 jebo23 걸그룹 트와이스가 K팝 여가수 최초로 빌보드위민인뮤직에서 수상했습니다소속사 JYP엔터테인먼트는 걸그룹 트와이스가 현지시간으로 지난 1일 미국 로스앤젤레스 유튜브 시어터에서 열린 빌보드위민인뮤직트와이스 K팝 여가수 최초 빌보드위민인뮤직 수상'),\n",
       " Row(idol='NewJeans', total_content=' 뉴진스에게 이리 애기애기하던 시절이 있었다니……이 영상 박제해요민희진 대표가 자신의 개인 계정에 뉴진스의 민지 혜인의 데뷔 전 영상을 올려 눈길을 끈다영상 속 민지와 혜인은 지금보다 훨씬 앳되보이는 모습 냉장고의 글자 자석을 이용해 뉴진스를 만드는 표정이 모든 것이 신기하고 의욕에 넘치는 시절을 보여주는 듯 생기가 넘친다이들은 두분한번일어나주시죠란 말에 냉장고 앞에 앉아있다가 벌떡 일어나 청바지 뒤가 보이게 포즈를 취했다 청바지엔 각기 다른 모양으로 뉴진스 레터링이 장식되어 있어 귀여움을 더했다한편 민희진 어도어ADOR 대표는 2023빌보드우먼인뮤직2023 BILLBOARD WOMEN IN MUSIC에 선정됐다23일현지시간 빌보드는 공식 홈페이지를 통해 빌보드우먼인뮤직 리스트를 발표했다 이 가운데 리스트에는 민희진 어도어 대표도 언급됐다빌보드 우먼 인 뮤직은 한 해 동안 음악 산업에 큰 영향을 끼친 여성 아티스트 크리에이터 프로듀서 경영진 등을 선정하는 리스트다빌보드는 뉴진스가 디토Ditto로 빌보드 글로벌 200에서 8위를 달성하는 등 세계 음악 시장 진출에 박차를 가하고 있다라며 민희진 대표의 어도어 경영 성과를 언급했다민희진 대표는 지난해 미국 버라이어티가 선정하는 글로벌영향력있는여성 리스트에 이름을 올린 바 있다이미지 원본보기사진 출처민희진 개인 계정이미지 원본보기사진 출처민희진 개인 계정 뉴진스에게 이리 애기애기하던 시절이 있었다니……이 영상 박제해요 트와이스 민희진 대표가 자신의 개인 계정에 뉴진스의 민지 혜인의 데뷔 전 영상을 올려 눈길을 끈다 영상 속 민지와 혜인은 지금보다 훨씬 앳되보이는 모습이미지 원본보기사진 출처민희진 개인 계정스포츠조선 이정혁 기자트와이스 트와이스 트와이스 트와이스 뉴진스 데뷔 전 모습 공개에 얼마나더어렸던거야 민지·혜인 의욕 넘치고 생기가 가득')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# origin_df.select(\"idol\").collect()\n",
    "# spark.createDataFrame(words_rdd).collect()\n",
    "import pyspark.sql.functions as F\n",
    "words_df = df.withColumn(\"total_content\", F.concat(F.col(\"content\"), F.col(\"summary\"), F.col(\"title\")))\\\n",
    "    .select(F.col(\"idol\"),F.col(\"total_content\"))\n",
    "df.withCoulmn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4503be19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 312 ms, sys: 118 ms, total: 430 ms\n",
      "Wall time: 1.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import pyspark\n",
    "import string\n",
    "from pecab import PeCab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import pandas_udf, monotonically_increasing_id, regexp_replace, sum, asc, \\\n",
    "                                   arrays_zip, col, explode, sort_array, struct, lit, PandasUDFType, coalesce \n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType, ArrayType\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# local[*] : 모든 코어를 사용하겠다, local[4] : 4개의 코어를 사용하겠다.\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[3]\")\\\n",
    "        .appName(\"WordCount\")\\\n",
    "        .config(\"spark.driver.memory\", \"2g\")\\\n",
    "        .getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d686f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아이돌 별 뉴스 기사 수\n",
    "\n",
    "data_name= \"test_csv\"\n",
    "df = spark.read.options(header='True', inferSchema='True', delimiter='|',quote='\"', multiLine=\"true\").option(\"encoding\", \"UTF-8\").csv(f\"file:///home/j8a507/cluster/news/{data_name}.csv\")\n",
    "\n",
    "df.groupBy(\"idol\").count().write.csv(\"idol_news_count\")\n",
    "\n",
    "# Replace \"/path/to/partitioned/data\" with the path to your partitioned data in HDFS\n",
    "hdfs_path = \"idol_news_count\"\n",
    "\n",
    "# Replace \"/path/to/local/directory\" with the path to your local directory\n",
    "local_path = \"file:///home/j8a507/cluster/news/idol_news_count\"\n",
    "\n",
    "# Download the partitioned directory from HDFS to your local machine\n",
    "subprocess.check_call([\"hdfs\", \"dfs\", \"-getmerge\", hdfs_path, local_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_total = spark.read.options(header='True', inferSchema='True').option(\"encoding\", \"UTF-8\").csv(f\"file:///home/j8a507/cluster/news/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad453d82",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(rdd, schema)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# explode the list of tuples\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m, explode(\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeywords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# count the frequency of each keyword\u001b[39;00m\n\u001b[1;32m     18\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py:4495\u001b[0m, in \u001b[0;36mtransform\u001b[0;34m(col, f)\u001b[0m\n\u001b[1;32m   4449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(col, f):\n\u001b[1;32m   4450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4451\u001b[0m \u001b[38;5;124;03m    Returns an array of elements after applying a transformation to each element in the input array.\u001b[39;00m\n\u001b[1;32m   4452\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4493\u001b[0m \u001b[38;5;124;03m    +--------------+\u001b[39;00m\n\u001b[1;32m   4494\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_higher_order_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mArrayTransform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py:4444\u001b[0m, in \u001b[0;36m_invoke_higher_order_function\u001b[0;34m(name, cols, funs)\u001b[0m\n\u001b[1;32m   4441\u001b[0m expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(expressions, name)\n\u001b[1;32m   4443\u001b[0m jcols \u001b[38;5;241m=\u001b[39m [_to_java_column(col)\u001b[38;5;241m.\u001b[39mexpr() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[0;32m-> 4444\u001b[0m jfuns \u001b[38;5;241m=\u001b[39m [_create_lambda(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m funs]\n\u001b[1;32m   4446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mColumn(expr(\u001b[38;5;241m*\u001b[39mjcols \u001b[38;5;241m+\u001b[39m jfuns)))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py:4444\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4441\u001b[0m expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(expressions, name)\n\u001b[1;32m   4443\u001b[0m jcols \u001b[38;5;241m=\u001b[39m [_to_java_column(col)\u001b[38;5;241m.\u001b[39mexpr() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[0;32m-> 4444\u001b[0m jfuns \u001b[38;5;241m=\u001b[39m [\u001b[43m_create_lambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m funs]\n\u001b[1;32m   4446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mColumn(expr(\u001b[38;5;241m*\u001b[39mjcols \u001b[38;5;241m+\u001b[39m jfuns)))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py:4416\u001b[0m, in \u001b[0;36m_create_lambda\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m   4408\u001b[0m argnames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   4409\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   4410\u001b[0m     _unresolved_named_lambda_variable(\n\u001b[1;32m   4411\u001b[0m         expressions\u001b[38;5;241m.\u001b[39mUnresolvedNamedLambdaVariable\u001b[38;5;241m.\u001b[39mfreshVarName(arg)\n\u001b[1;32m   4412\u001b[0m     )\n\u001b[1;32m   4413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m argnames[: \u001b[38;5;28mlen\u001b[39m(parameters)]\n\u001b[1;32m   4414\u001b[0m ]\n\u001b[0;32m-> 4416\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Column):\n\u001b[1;32m   4419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf should return Column, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(result)))\n",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(rdd, schema)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# explode the list of tuples\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m, explode(transform(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# count the frequency of each keyword\u001b[39;00m\n\u001b[1;32m     18\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:463\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn is not iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "# define schema\n",
    "schema = StructType([\n",
    "    StructField(\"keywords\", StringType(), True),\n",
    "    StructField(\"index\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# create sample data\n",
    "data = ([('a', 5),('b',4)], 0), ([('c', 3),('d',2)], 1)\n",
    "\n",
    "# convert to PySpark DataFrame\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "# explode the list of tuples\n",
    "df = df.withColumn(\"keywords\", explode(transform(col(\"keywords\"), lambda x: struct(*x))))\n",
    "\n",
    "# count the frequency of each keyword\n",
    "df = df.groupBy(\"keywords\").count()\n",
    "\n",
    "# sort by count in descending order\n",
    "df = df.orderBy(\"count\", ascending=False)\n",
    "\n",
    "# show result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a82cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61cfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
